<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Natural Language to LoRA Weights | Devesh Pant </title> <meta name="author" content="Devesh Pant"> <meta name="description" content="llm, research paper"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="preconnect" href="https://fonts.googleapis.com"> <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin> <link href="https://fonts.googleapis.com/css2?family=Crimson+Text:ital,wght@0,400;0,600;1,400&amp;family=Source+Serif+Pro:ital,wght@0,400;0,600;1,400&amp;family=Source+Code+Pro:wght@400;500&amp;display=swap" rel="stylesheet"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pantdevesh.github.io/posts/2025/07/02/text2lora/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Devesh</span> Pant </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/random/">random </a> </li> <li class="nav-item "> <a class="nav-link" href="/notes/index.html">notes </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Natural Language to LoRA Weights</h1> <p class="post-meta"> Created on July 02, 2025 </p> <p class="post-tags"> <i class="fa-solid fa-calendar fa-sm"></i> 2025   ·   <i class="fa-solid fa-hashtag fa-sm"></i> formatting   <i class="fa-solid fa-hashtag fa-sm"></i> links   ·   <i class="fa-solid fa-tag fa-sm"></i> notes </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Recently Sakana AI released a paper called <a href="https://github.com/SakanaAI/text-to-lora" rel="external nofollow noopener" target="_blank">Text-to-Lora</a>, which proposes generating LoRA weights directly from natural language task descriptions.</p> <h3 id="challenges-with-finetuning">Challenges with Finetuning</h3> <p>In NLP, large pre-trained models can be used for various downstream tasks such as Question Answering, Named Entity Recognition and translation. However, these models often require task specific adaptation. Fully finetuning the model for each task is computationally expansive and may degrade its performance on other tasks. To address this, researchers explored methods such as finetuning only a subset of parameters or adding lightweight task specific modules, though these approaches had their own limitatinos. In 2021, researchers from Microsoft introduced LoRA (Low Rank Adaptation) which quickly gained widespread adoption not only in language models but also in domains such vision transformers and diffusion models.</p> <h3 id="lora-low-rank-adaptation">LoRA (Low Rank Adaptation)</h3> <p>LoRA efficiently finetunes pre-trained models by decomposing weight updates into low-rank matrices. Building upon the idea that weights of a large pre-trained model have low-intrinsic rank, the authors hypothesized that even the weight updates should also have a low intrinsic-rank. <br>If $W_0 \in \mathbb{R}^{d \times d}$ is a frozen pre-trained weight, the update is parameterized as $\Delta W = BA$ where $A \in \mathbb{R}^{r \times d}, B \in \mathbb{R}^{d \times r}$ with $r \ll d$. Only $A, B$ are trained, reducing trainable parameter count significantly. Assume $W_{0}$ has dim: 512x512, if rank is 8 then the A and B has dim: 512x8 and 8x512, i.e. 32 times less parameters than $W_{0}$. Also, there is no additional inference time latency.</p> <h4 id="challenges-in-lora">Challenges in LoRA</h4> <p>The papers highlight following challenges with LoRA:</p> <ul> <li>A new LoRA adapter need to be trained for each downstream task.</li> <li>Though reduced trainable parameters, training LoRA adapter still takes significant time.</li> </ul> <h4 id="text-to-lora">Text-to-Lora</h4> <p>— <em>Sakana AI</em><br> In this paper authors raise these questions– <br></p> <ol> <li>Can we train neural networks to generate LoRA?</li> <li>Can these neural networks generate new LoRAs from unseen task descriptions during test-time?</li> </ol> <p>Generating LoRAs from natural task descriptors:<br> Training Data: <br> N datasets ${D_1, D_2, \ldots, D_n}$ each having a task descriptor $T_i$, that contains a general description of the dataset. <br> Training Objective for $task_i:\; \Delta W_i = \arg\min \; Loss_{\text{sft}}(D_i, W_0, \Delta W_i)$</p> <h4 id="hypernetworks">Hypernetworks</h4> <p>The network used to synthesize the weights of the LoRA. Hypernetwork were proposed in 2016 to generate parameters of another neural network. Hypernetworks in a way compresses different neural networks within it’s weights, given size of it’s parameters is much smaller than those networks it is synthesizing. Formally, given a layer descriptor vector $v_l$, hypernetwork generates parameter of layer $l: W_l = h(v_l)$. The network is trained end-to-end on a downstream task.</p> <h3 id="text-to-lora-using-hypernetwork">Text-to-Lora using Hypernetwork</h3> <p>Given a target module (m) and layer index (l), text2Lora uses a hypernetwork to generate low-rank matrices A and B, based on a task descriptor $z^{i}\in Z^{i}$ of task $t^{i}$ :<br> \(\Delta W_{m,l}^{i} = h_{\theta}(\phi_{m,l}^{i})\)</p> <blockquote> <p><strong>Note:</strong>: Task embedding can be one-hot encoding or a learnable embedding, however one-hot encoding may lead to generalization issues.</p> </blockquote> <p>\(\phi_{m,l}^{i} = \text{concat}[f(z^{i}), E[m], E[l]]\) Where f is a function to generate embedding of the task description, typically $\text{[CLS]}$ token of a bi-directional transformer or last token activation of an LLM.<br> E is a learnable embedding dictionary indexed by either module $m$ or layer index $l$. <br> Supervised finetuning objective for T2L is: \(\Theta = \arg\min_{\theta}(\mathcal{L}_{\mathrm{SFT}}(D^{i},\psi,h_{\theta}(\phi^{i})))\)</p> <blockquote> <p><strong>Note:</strong>: We can batch $m$ and $l$, that allows us to generate the parameters of all modules and layers in a single forward pass.</p> </blockquote> <h4 id="different-hypernetwork-architectures">Different Hypernetwork Architectures:</h4> <p>Authors propose three architectures: L (Large), M (Medium) and S (Small).</p> <ul> <li> <p><strong>L Architecture</strong>: Final layer outputs $A$ and $B$ matrices simultaneously. Output parameters: \(|\theta_{head}| = d_{head}\times 2 \times r \times d\) where $d_{head}$ is the dimension of the last MLP, $r$ is the rank of the matrices $A$ and $B$ and $d$ is the dimension of weight matrix $W$.</p> </li> <li> <p><strong>M Architecture</strong>: Final layer outputs either $A$ and $B$ matrix depending on the input embedding, meaning that the output layer is shared between $A$ and $B$. Output parameters: \(|\theta_{head}| = d_{head} \times r \times d\)</p> </li> <li> <p><strong>S Architecture</strong>: Final layer outputs $A$ and $B$ matrices simultaneously. Output parameters: \(|\theta_{head}| = d_{head}\times d\) where $d_{emb}$ is the dimension of weight matrix $W$. This variation has strongest inductive bias.</p> </li> </ul> <p>All of these architectures can generate all LoRAs in a single forward pass by batching the input text embeddings.</p> <h4 id="training-of-hyperparameters">Training of Hyperparameters</h4> <p>Authors propose two ways of training the hypernetwork:</p> <ul> <li>by reconstructing trained LoRAs</li> <li>supervised finetuning on downstream task.</li> </ul> <ol> <li> <strong>Training Text-to-LoRA via LoRA Reconstruction:</strong> The network is trained to reconstruct LoRA matrices. We can either use LoRAs from a pre-trained library or first perform peft training to generate LoRAs than train our hypernetwork. T2L can be trained using either one-hot vector embeddings or task description in natural languages. However, there is a downside with training using one-hot embeddings, as we won’t be able to use the network in a zero-shot setting. <br> Given a suitable library of LoRA adapters $\Omega$, the reconstruction loss for T2L can be written as: \(\mathcal L(\Omega,\theta) = \mathbb E_{\Delta W^i \sim \Omega} (\Delta W^i - h_{\theta}(\phi^i))\)</li> <li> <strong>Training T2L via end-to-end finetuning:</strong> One issue with the prior approach is if we have reconstructed two LoRAs related to similar tasks, whose weights $\Delta W1$ and $\Delta W2$ reside in different minima, then trained hypernetwork might not generalize on the similar tasks. We can avoid it by directly finetuning T2L on the target task, so that it implicitly learns to cluster the similar LoRAs.</li> </ol> <h3 id="experiments-and-results">Experiments and Results</h3> <p>Model: Mistral-7b-Instruct <br> Dataset: Supernatural Instruction Dataset (\cite https://huggingface.co/datasets/andersonbcdefg/supernatural-instructions-2m)</p> <p><strong>Experiment 1:</strong> Aim: Whether T2L can recover the performance of task specific LoRAs when trained with reconstruction loss. Task-specific LoRAs are trained on the 9 benchmark datasets, creating a library of LoRAs and Hypernetwork is trained via reconstruction loss using these. The results show it can effectively distill the LoRAS and even outperforms LoRA weights on multiple tasks (likely due to implicit regularization from lossy compression of LoRAs)</p> <p><strong>Experiment 2:</strong> Aim: Whether T2L generate LoRAs in zero-shot setting T2L is trained via SFT on 479 tasks from the SNI dataset. Multiple Task descriptions are generated using GPT-4o-mini and sampled in an online fashion. The results on the evaluation set show that model outperforms various other approaches including multi-task LoRA, but falls short of task specific LoRA on several benchmarks. It does, however, outperforms task specific LoRA on certain benchmarks.</p> <h3 id="limitations-and-future-work">Limitations and Future Work</h3> <ul> <li>Performance gap in zero-shot setting</li> <li>Sensitive to task descriptions</li> <li>Requires high quality task descriptions (generated via gpt-4o-mini for the experiments)</li> </ul> <h3 id="related-works-and-references">Related Works and References</h3> <ol> <li> <a href="https://jerryliang24.github.io/DnD" rel="external nofollow noopener" target="_blank">Drag and Drop LLMs (DnD)</a>, which utilizes slightly differnet architecture for the same task of text-to-lora generation.</li> <li><a href="https://arxiv.org/abs/2203.08304" rel="external nofollow noopener" target="_blank">Hyperdecoders</a></li> <li> <a href="https://aclanthology.org/2024.findings-emnlp.956.pdf" rel="external nofollow noopener" target="_blank">HyperLoRA</a> - Very similar framework but, along with the task description few shot examples are also provided.</li> <li><a href="https://arxiv.org/abs/1609.09106" rel="external nofollow noopener" target="_blank">Hypernetworks</a></li> <li><a href="https://openreview.net/forum?id=O0iQkpQfFe" rel="external nofollow noopener" target="_blank">Hypertuning</a></li> </ol> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2025/08/14/useful-links/">Curated List of Useful Blogs and Videos</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2025/03/26/plotly/">a post with plotly.js</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/posts/2024/12/04/photo-gallery/">a post with image galleries</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Devesh Pant. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>