---
layout: post
title: Natual Language to LoRA Weights
date: 2025-07-02 22:20:16
description: llm, research paper
tags: formatting links
categories: notes
featured: true
---
Recently Sakana AI released a paper called [Text-to-Lora](https://github.com/SakanaAI/text-to-lora) to generate LoRA weights from the natural language task descriptions. There was another similar paper released called [Drag and Drop LLMs (DnD)](https://jerryliang24.github.io/DnD) which proposes a different framework for a similar task. 
In this blog we will understand the working of these two approaches. Let us first understand the challenges with model finetuning for downstream tasks.
In NLP (Natural Language Processing), a pre-trained language model is used for multiple downstream tasks. For example, T4 can be used for Question Answering, Named Entity Recognition, translation and many other tasks. However, if we want to finetune a large pre-trained model for a specific downstream application, then it would take lots of compute and might break the pre-trained model's performance on other tasks. That is why, researchers came with an idea of finetuning only few parameters or adding additional task specific modules. There were lot's of ideas around this, however in 2021, researchers from Microsoft proposed LoRA (Low Rank Adaptation) which gained widespread adoption not only for language models but across domains such vision transformers and diffusion models. (Existing Methods: Task specific adapters cause latency. Prefix finetuning is no good. )
### LoRA (Low Rank Adaptation) 
LoRA is a method to efficiently finetune pre-trained models. Building upon the idea that weights of a large pre-trained model have low-intrinsic rank, the authors hypothesize that even the weight updates also have a low intrinsic-rank.  They decompose the update to the pre-trained weight matrix del W into low rank decomposition: W0 + del(W) = W0 + BA <br>
A forward pass would look like: h = W0x + del(w)x ===> W0x + BAx
Gradient updates are required only for the low rank matrix.
Assume W0 has dim: 512x512, than A and B has dim: 512 x 8 and 8 x 512, thats 32 times less parameters than W0, now imagine that will the differnt weight matrices in the network. 
(No inference time latency )

#### Challenges with LoRA
The papers highlight following challenges with LoRA:
- A new LoRA adapter need to be trained for each downstream task
- Though reduced trainable parameters, training LoRA adapter still takes significant time


#### Text-to-Lora 
---         *Sakana AI*
The authors try to answer two questions-- <br>
1. Can we train neural networks to generte LoRA?
2. Can these neural networks generate new LoRAs from unseen task descriptions during test-time?


##### Generating LoRAs from natural task descriptors
Training Data: <br>
N datasets {D1, D2, ..., Dn} each having a task descriptor Ti, which contains a general description of the dataset. <br>
Training Objective for task_i: del(W)_i = argmin Loss_sft(Di, pretrained_llm, del(W)_i).

#### Hypernetworks
The network used to synthesize the weights of the LoRA. Hypernetwork (2016) is used to generate parameters of another network or kind of compress different neural networks within it's weights. Formally, given a layer descriptor vector v_l, hypernetwork generates parameter of layer l: W_l = h(v_l). The network is trained end-to-end on a downstream task. 

### Text-to-Lora using Hypernetwork
Given a target module (m) and layer index (l), text2Lora uses a hypernetwork to generate low-rank matrices A and B, based on a task descriptor $z^{i}\in Z^{i}$ of task $t^{i}$ :<br>
$$
\Delta W_{m,l}^{i} = h_{\theta}(\phi_{m,l}^{i})
$$

$$
\phi_{m,l}^{i} = \text{concat}[f(z^{i}), E[m], E[l]]
$$
Where f is a function to generate embedding of the task description, typically $\text{[CLS]}$ token of a bi-directional transformer or last token activation of an LLM.<br>
E is a learnable embedding dictionary indexed by either module $m$ or layer index $l$. <br>
Supervised finetuning objective for T2L is:
$$
\Theta = \arg\min_{\theta}(\mathcal{L}_{\mathrm{SFT}}(D^{i},\psi,h_{\theta}(\phi^{i})))
$$
>**Note:**: We can batch $m$ and $l$, that allows us to generate the parameters of all modules and layers in a single forward pass. 

#### Different Hypernetwork Architectures:
Authors propose three architecures: L (Large), M (Meidum) and S (Small). 
- **L Architecture**: Final layer outputs $A$ and $B$ matrices simultaneously. Output parameters: 
  $$
  |\theta_{head}| = d_{head}\times 2 \times r \times d 
  $$
  where $d_{head}$ is the dimension of the last MLP, $r$ is the rank of the matrices $A$ and $B$ and $d$ is the dimension of weight matrix $W$.

- **M Architecture**: Final layer outputs either $A$ and $B$ matricx depending on the input embedding, meaning that the output layer is shared between $A$ and $B$. Output parameters: 
  $$
  |\theta_{head}| = d_{head} \times r \times d 
  $$

- **S Architecture**: Final layer outputs $A$ and $B$ matrices simultaneously. Output parameters: 
  $$
  |\theta_{head}| = d_{head}\times d
  $$
  where $d_{emb}$ is the dimension of weight matrix $W$. This variation has strongest inductive bias. 

All of these architecures can generate all LoRAs in a single forward pass by batching the input text embeddings. 

#### Training of Hyperparameters
There are two possible ways of training the hypernetwork. One is by reconstructing trained LoRAs and other is via Supervised Finetuning. 
- **Training Text-to-LoRA via LoRA Reconstruction:** The network is trained to reconstruct LoRA matrices. We can either use LoRAs from a pre-trained library or first perform peft training to generate LoRAs than train our hypernetwork. T2L can be trained using either one-hot vector embeddings or task description in natural languages. However, there is a downside with training using one-hot embeddings, as we won't be able to use the network in a zero-shot setting. <br>
Given a suitable library of LoRA adapters $\Omega$, the reconstruction loss for T2L can be written as:
$$
\mathcal L(\Omega,\theta) = \mathbb E_{\Delta W^i \sim \Omega} (\Delta W^i - h_{\theta}(\phi^i))
$$
- **Training T2L via end-to-end finetuning:**
One issue with the prior approach is if we have reconstructed two LoRAs related to similar tasks, whose weights $\Delta W1$ and $\Delta W2$ reside in different minima, then trained hypernetwork might not generalize on the similar tasks. We can avoid it by direclty finetuning T2L on the target task, so that it implicitly learns to cluster the similar LoRAs. 

Jean shorts raw denim Vice normcore, art party High Life PBR skateboard stumptown vinyl kitsch. Four loko meh 8-bit, tousled banh mi tilde forage Schlitz dreamcatcher twee 3 wolf moon. Chambray asymmetrical paleo salvia, sartorial umami four loko master cleanse drinking vinegar brunch. [Pinterest](https://www.pinterest.com) DIY authentic Schlitz, hoodie Intelligentsia butcher trust fund brunch shabby chic Kickstarter forage flexitarian. Direct trade <a href="https://en.wikipedia.org/wiki/Cold-pressed_juice">cold-pressed</a> meggings stumptown plaid, pop-up taxidermy. Hoodie XOXO fingerstache scenester Echo Park. Plaid ugh Wes Anderson, freegan pug selvage fanny pack leggings pickled food truck DIY irony Banksy.

#### Hipster list

- brunch
- fixie
- raybans
- messenger bag

#### Check List

- [x] Brush Teeth
- [ ] Put on socks
  - [x] Put on left sock
  - [ ] Put on right sock
- [x] Go to school

Hoodie Thundercats retro, tote bag 8-bit Godard craft beer gastropub. Truffaut Tumblr taxidermy, raw denim Kickstarter sartorial dreamcatcher. Quinoa chambray slow-carb salvia readymade, bicycle rights 90's yr typewriter selfies letterpress cardigan vegan.

<hr>

Pug heirloom High Life vinyl swag, single-origin coffee four dollar toast taxidermy reprehenderit fap distillery master cleanse locavore. Est anim sapiente leggings Brooklyn ea. Thundercats locavore excepteur veniam eiusmod. Raw denim Truffaut Schlitz, migas sapiente Portland VHS twee Bushwick Marfa typewriter retro id keytar.

> We do not grow absolutely, chronologically. We grow sometimes in one dimension, and not in another, unevenly. We grow partially. We are relative. We are mature in one realm, childish in another.
> â€”Anais Nin

Fap aliqua qui, scenester pug Echo Park polaroid irony shabby chic ex cardigan church-key Odd Future accusamus. Blog stumptown sartorial squid, gastropub duis aesthetic Truffaut vero. Pinterest tilde twee, odio mumblecore jean shorts lumbersexual.
